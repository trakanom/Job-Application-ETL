{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import datasets\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import email\n",
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "from models.config.cleaning_methods import decode_mime_stuff\n",
    "from datetime import datetime as dt\n",
    "from models.config.filter_methods import testing_platform_filters\n",
    "from models.config.CONSTS import DATE_FORMAT\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stripped down version of the platform parsing methods dict.\n",
    "Creating functions to streamline data creation.\n",
    "'''\n",
    "\n",
    "\n",
    "platform_identification = {\n",
    "    \"linkedin.com\":\"LinkedIn\",\n",
    "    \"untapped.gg\":\"Untapped\",\n",
    "    \"ziprecruiter.com\":\"ZipRecruiter\",\n",
    "    \"squarepeg.com\":\"SquarePeg\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "platform_filters = {\n",
    "    'LinkedIn' : {\n",
    "        'email': {\n",
    "            'match' : lambda x : \"X-LinkedIn-Template\" in x['headers'].keys(),\n",
    "            'scan' : {\n",
    "                'date'        : lambda x : dt.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime(DATE_FORMAT), #standard to gmail format; gets datetime of email sent\n",
    "                'update_type' : lambda x : re.search(r'jobs?_appli\\w{4,6}_([a-z]+)', x['headers']['X-LinkedIn-Template']).group(1), #type of email update\n",
    "                'subject'     : lambda x : decode_mime_stuff(x['headers']['Subject']).replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\").split(\":\")[0],\n",
    "                # 'title' : lambda x : re.search(x,r''), #title of job posting\n",
    "                # 'company' : lambda x : re.search(x,r''), #company name\n",
    "            },\n",
    "            'applied' : {\n",
    "                'PostID'   : lambda x : re.search(r\"(\\d{10})\",x['body'].find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "                'url'      : lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\",x['body'].find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1)),\n",
    "                'position' : lambda x : x['subject'].split(\" at \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" at \")[1],\n",
    "            },\n",
    "            'viewed' : {\n",
    "                'PostID'   : lambda x : re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim'), x['body'].find(\"a\", href=re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim'))['href']).group(1), #re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', flags = re.DOTALL), x['body']).group(1),\n",
    "                'position' : lambda x : x['subject'].split(\" was viewed by \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" was viewed by \")[1],\n",
    "            },\n",
    "            'rejected' : {\n",
    "                # 'original_date_applied' : lambda x: dt.strptime(re.search(re.compile(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\"), x['body']).group(1),'%B %d, %Y').strftime(DATE_FORMAT),\n",
    "                'position' : lambda x : x['subject'].split(\" at \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" at \")[1],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'TEMPLATE' : {\n",
    "        'email': {\n",
    "            'match' : lambda x: x,\n",
    "            'scan' : {\n",
    "                'date'        : lambda x: x,\n",
    "                'update_type' : lambda x: x,\n",
    "                'subject'     : lambda x: x,\n",
    "                # 'title' : lambda x: x,\n",
    "                # 'company' : lambda x: x,\n",
    "            },\n",
    "            'applied' : {\n",
    "                'PostID'   : lambda x: x,\n",
    "                'url'      : lambda x: x,\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "            'viewed' : {\n",
    "                'PostID'   : lambda x: x,\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "            'rejected' : {\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "def cleaning_function_decorator(func):\n",
    "    '''\n",
    "    Wrapper that cleans all strings inside a list, dict, or string-like object.\n",
    "    Converts numbers into float or int if applicable.\n",
    "    Returns an object of the same format, but with cleaned strings.\n",
    "    \n",
    "    '''\n",
    "    def wrapper(*args, **kwargs):\n",
    "        results = func(*args,**kwargs)\n",
    "        return clean_data(results)\n",
    "    \n",
    "    def clean_string(text):\n",
    "        replace_chars = [('\\n',\" \"), (re.compile(r\"\\s+\"),\" \")]\n",
    "        results = text\n",
    "        for item in replace_chars:\n",
    "            results = re.sub(item[0],item[1], results)\n",
    "        results = results.strip()\n",
    "        return results\n",
    "    \n",
    "    def clean_data(input_item):\n",
    "        '''\n",
    "        Recursive function that cleans strings inside objects.\n",
    "        '''\n",
    "        type_list = {\"int\":int, \"str\":str, \"dict\":dict, \"list\":list,}\n",
    "        result_type = max([label if isinstance(input_item, type_example) else \"\" for (label,type_example) in type_list.items()]) #creates a list that checks the type of the input and returns a string representation of the type. Mostly using this so I can use match/case instead of an If/Else ladder.\n",
    "        match result_type:\n",
    "            case \"str\":\n",
    "                print(\"string\")\n",
    "                if len(input_item)<256 and re.search(r\"\\d\",input_item): #Typically not working with bigint, but will change as needed.\n",
    "                    try: #This is going to hurt performance, but this will allow numerical strings to be relabeled as int or float\n",
    "                        try:\n",
    "                            return int(input_item.strip(\",\"))\n",
    "                        except:\n",
    "                            return float(input_item.strip(\",\"))\n",
    "                    except:\n",
    "                        return clean_string(input_item)\n",
    "                return clean_string(input_item)\n",
    "            case \"dict\":\n",
    "                print(\"dictionary\")\n",
    "                input_item = {label:clean_data(item) for (label,item) in input_item.items()}\n",
    "            case \"list\":\n",
    "                print(\"list\")\n",
    "                return [clean_data(item) for item in input_item]\n",
    "            case \"int\":\n",
    "                print(\"int\")\n",
    "            case \"float\":\n",
    "                print(\"float\")\n",
    "            case _:\n",
    "                print(result_type)\n",
    "        return input_item\n",
    "    return wrapper\n",
    "\n",
    "def handle_comprehension_errors(input_data, method_list):\n",
    "    '''\n",
    "    Error handling for dictionary comprehensions.\n",
    "    usage:\n",
    "    data|=handle_comprehension_errors(inputdata, testing_platform_filters['LinkedIn']['selenium-client']['post'])\n",
    "    '''\n",
    "    \n",
    "    output_data = input_data\n",
    "    if 'errors' not in input_data.keys():\n",
    "        output_data|={'errors':{}}\n",
    "    for data_label in method_list: #loops through each data_label \n",
    "        try:\n",
    "            output_data |= {data_label : method_list[data_label](input_data)} #applies filter to captured data and inserts the key-value pair into input_data\n",
    "        except Exception as e:\n",
    "            output_data |= {data_label : None} #If error, fills in value with null\n",
    "            output_data[\"errors\"] |= {data_label:e} #\n",
    "    return output_data\n",
    "\n",
    "\n",
    "def load_emails(directory):\n",
    "    cached_emails = []\n",
    "    file_list = os.listdir(directory)\n",
    "    file_list = [file for file in file_list if \".eml\" in file] #file not in read_files and \n",
    "    for item in file_list:\n",
    "        path = directory+item\n",
    "        with open(path, encoding=\"utf-8\") as file:\n",
    "            results = email.message_from_file(file)\n",
    "            if results.is_multipart:\n",
    "                # results = results.get_payload()\n",
    "                body = results.get_payload()[1]\n",
    "                print(\"Multipart\")\n",
    "            else:\n",
    "                # results = results.get_payload()\n",
    "                body = results.get_payload()\n",
    "                print(\"Not Multipart\")\n",
    "            # print(results)\n",
    "            body = re.sub(r\"=$\\n\",\"\", body.as_string(), flags=re.MULTILINE)\n",
    "            headers = {part:re.sub(r\"\\n\\s+\",\" \",value) for (part,value) in results.items()}\n",
    "            html_contents = BeautifulSoup(body, \"html5lib\")\n",
    "            data = { \n",
    "                'subject': headers['Subject'],\n",
    "                'headers': headers,\n",
    "                'body': html_contents.text,\n",
    "                'from': headers['From']\n",
    "            }\n",
    "            \n",
    "            cached_emails.append(data)\n",
    "    return cached_emails\n",
    "\n",
    "    \n",
    "            # print(headers.keys())\n",
    "\n",
    "def parse_email(data):\n",
    "    input_method = 'email'\n",
    "\n",
    "    # print(data['headers']['Subject'])\n",
    "    sender = data['from'].split(\"@\")[1].strip(\">\")\n",
    "    platform = platform_identification[sender] if sender in platform_identification.keys() else (data['platform'])\n",
    "    # platform = 'LinkedIn'\n",
    "    filters = platform_filters[platform][input_method]\n",
    "    # filters = testing_platform_filters[platform][input_method]\n",
    "    \n",
    "    # try:\n",
    "    if filters['match'](data):\n",
    "        data|=handle_comprehension_errors(data, filters['scan'])\n",
    "        data|=handle_comprehension_errors(data, filters[data['update_type']])\n",
    "            \n",
    "            # data |= {label:method(data) for (label,method) in filters['scan'].items()} #Initializes date, update_type, and subject\n",
    "            # data |= {label:method(data) for (label,method) in filters[data['update_type']].items()} #processes the above for more data\n",
    "    # except Exception as e:\n",
    "    #     print(e, \"!!!\", data['subject'])\n",
    "    #     print(\"Body: \", data['body'])\n",
    "    return data \n",
    "\n",
    "\n",
    "@cleaning_function_decorator\n",
    "def parse_emails(input_data):\n",
    "    results = [parse_email(item) for item in input_data]\n",
    "    return results\n",
    "\n",
    "\n",
    "    '''\n",
    "    data = {\n",
    "        \"date:\":headers['Received'].split(\";\")[1],\n",
    "        \"update_type\": re.search(r'jobs?_appli\\w{4,6}_([a-z]+)',headers['X-LinkedIn-Template']).group(1), \n",
    "        \"JobID\": re.search(r\"(\\d{10})\",html_contents.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "        \"Subject\": headers['Subject'].replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\"), #re.search(r\"Your? appli(cation|ed) (for|to) (.+)( at|was viewed by)(.+)\",headers['Subject']), \n",
    "        \"From\":headers['From'],\n",
    "        \"To\":headers['To'],\n",
    "        # \"Body\":body,\n",
    "    }\n",
    "    # match data['update_type']:\n",
    "    #     case 'applied':\n",
    "    #         print(body)\n",
    "    #         jobID = re.search( r'View job: https:(\\S+&jobId=3D(\\w+))' , html_contents, flags = re.DOTALL ).group(2)\n",
    "    #     case 'viewed':\n",
    "    #         jobID = re.search(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', html_contents, flags = re.DOTALL).group(1)\n",
    "    #     case 'rejected':\n",
    "    #         jobID = None\n",
    "    #         # find jobID via match, perhaps. Maybe at later stage?\n",
    "    #     case _:\n",
    "    #         jobID = None\n",
    "            \n",
    "        \n",
    "\n",
    "    # subject_filtering = re.search(re.compile(\"Your? appli(cation|ed) (for|to) (.+)( at|was viewed by)(.+)\",flags=re.DOTALL),decode_mime_stuff(headers['Subject']))\n",
    "    split_string = \" was viewed by \" if data['update_type']==\"viewed\" else \" at \"\n",
    "    position, company = decode_mime_stuff(headers['Subject']).split(split_string)\n",
    "    more_data = {\n",
    "        \"company\": company, #subject_filtering.group(5),\n",
    "        \"position\": position, #subject_filtering.group(3),\n",
    "        \"PostURL\": 'https://www.linkedin.com/jobs/view/{}/'.format(data['JobID'])\n",
    "    }\n",
    "    # \"JobID\": re.search(r\"(\\d{10})\",html_contents.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cleaning_function_decorator\n",
    "def test_function(input_dict):\n",
    "    # print(input_dict)\n",
    "    return input_dict\n",
    "\n",
    "test_results = [{\"Test\":[\"123\",\"420.69\", \"Test123\"],\"Frog\":\"15\", \"Returns4\":lambda x: \"4\",\"4\":\"12\"}, {\"Frog\":\"12\"} ]\n",
    "\n",
    "cleaned_results = test_function(test_results)\n",
    "try:\n",
    "    assert isinstance(cleaned_results[0][\"Test\"][1], float), \"FAILED: Float not converted\"\n",
    "    assert isinstance(cleaned_results[0][\"Test\"][0], int), \"FAILED: Int not converted\"\n",
    "    assert cleaned_results[0]['Returns4'](9999)==\"4\", \"FAILED: Lambda converted\"\n",
    "    print(\"PASSED\")\n",
    "    print(cleaned_results)\n",
    "except AssertionError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmails(credpath, filters = None, query=None, nextPage=None, maxResults=None, limit=None):\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    maxResults=500 if maxResults is None else maxResults\n",
    "    cached_data = []\n",
    "    duplicates = {}\n",
    "    rejected_values = [] \n",
    "    # Variable creds will store the user access token.\n",
    "    # If no valid token found, we will create one.\n",
    "    creds = None\n",
    "    pickle_path = '.\\\\credentials\\\\token.pickle'\n",
    "    pickle_path = re.search(r\"(\\.(\\\\\\w+)+)\\\\\\w+.json\", credpath).group(1)+r\"\\\\token.pickle\"\n",
    "    # The file token.pickle contains the user access token.\n",
    "    # Check if it exists\n",
    "    if os.path.exists(pickle_path):\n",
    "\n",
    "        # Read the token from the file and store it in the variable creds\n",
    "        with open(pickle_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If credentials are not available or are invalid, ask the user to log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and not creds.refresh_token:\n",
    "            # os.remove(credpath)\n",
    "            \n",
    "            creds.refresh(Request()) #TODO Handle invalid/expired credentials properly. TODO Need a refresh token or some other \n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(f'{credpath}', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        # Save the access token in token.pickle file for the next run\n",
    "        with open(pickle_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    # Connect to the Gmail API\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "    # request a list of all the messages\n",
    "    if nextPage==None:\n",
    "        result = service.users().messages().list(userId='me', maxResults=maxResults, q=query).execute()\n",
    "    else:\n",
    "        result = service.users().messages().list(userId='me', maxResults=maxResults, q=query, pageToken=nextPage).execute()\n",
    "    \n",
    "    # We can also pass maxResults to get any number of emails. Like this:\n",
    "    # result = service.users().messages().list(maxResults=200, userId='me').execute()\n",
    "    messages = result.get('messages')\n",
    "    number_messages = len(messages)\n",
    "    print(\"Number of messages for this round: \", number_messages)\n",
    "    # messages is a list of dictionaries where each dictionary contains a message id.\n",
    "\n",
    "            \n",
    "    if isinstance(limit, int):\n",
    "        messages = messages[:min(limit, len(messages))]\n",
    "    # iterate through all the messages\n",
    "    for msg in messages:\n",
    "        # Get the message from its id\n",
    "        txt = service.users().messages().get(userId='me', id=msg['id']).execute()\n",
    "        # Use try-except to avoid any Errors\n",
    "        # try:\n",
    "        # Get value of 'payload' from dictionary 'txt'\n",
    "        payload = txt['payload'] # TODO Start Point\n",
    "        headers = payload['headers']\n",
    "        headers = {item['name']:item['value'] for item in headers}\n",
    "        # Look for Subject and Sender Email in the headers\n",
    "        \n",
    "        platform_key = \"LinkedIn\"\n",
    "        target_header = 'X-LinkedIn-Template'\n",
    "        target_values = ['jobs_applicant_applied', 'email_jobs_job_application_viewed_01', 'email_jobs_application_rejected_01'] # TODO Probably create a classification dict\n",
    "        \n",
    "        filter_stuff = target_header in headers.keys() #eventually call all email classification methods #TODO undo hardcoded linkedin\n",
    "        \n",
    "        \n",
    "        \n",
    "        if filter_stuff: # Checks if \n",
    "            if headers[target_header] not in target_values:\n",
    "                rejected_values.append(headers[target_header])\n",
    "                continue\n",
    "            print(\"FILTER RESULT\", headers[target_header])\n",
    "        else:\n",
    "            continue\n",
    "        update_type = headers[target_header]\n",
    "        # The Body of the message is in Encrypted format. So, we have to decode it.\n",
    "        # Get the data and decode it with base 64 decoder.\n",
    "        parts = payload.get('parts')[0]\n",
    "        data = parts['body']['data']\n",
    "        data = data.replace(\"-\",\"+\").replace(\"_\",\"/\")\n",
    "        decoded_data = base64.b64decode(data)\n",
    "\n",
    "        # Now, the data obtained is in lxml. So, we will parse it with BeautifulSoup library\n",
    "        soup = BeautifulSoup(decoded_data , \"lxml\")\n",
    "        body = soup.body()[0].text\n",
    "        data = {\n",
    "            \"date_scanned\": dt.now().strftime(DATE_FORMAT),\n",
    "            \"subject\":headers['Subject'],\n",
    "            \"headers\":headers,\n",
    "            \"platform\": platform_key, # TODO create a matching method. Perhaps just platform identification.\n",
    "            \"body\":BeautifulSoup(base64.b64decode(payload.get('parts')[0]['body']['data'].replace(\"-\",\"+\").replace(\"_\",\"/\")), \"lxml\").body()[0].text,\n",
    "            \"from\":headers['From'],\n",
    "        }\n",
    "        \n",
    "        cached_data.append(data)\n",
    "    # except Exception as e:\n",
    "    # \tprint(e,'\"EXCEPTION')\n",
    "    rejected_values = set(rejected_values)\n",
    "    # print(cached_data)\n",
    "    print(\"Duplicates\", duplicates)\n",
    "    print(\"Unfiltered Emails\", rejected_values)\n",
    "    if number_messages == maxResults and (limit==None or maxResults<limit): #capped query; need to rerun until all messages are parsed.\n",
    "        print(\"Next round of parsing!\")\n",
    "        cached_data.extend(getEmails(credpath=credpath, filters=filters, query=query, nextPage=result['nextPageToken'], maxResults=maxResults, limit=None if limit==None else limit-maxResults)) # Creating a union between these sets. Allows for recursion.\n",
    "    \n",
    "    return cached_data\n",
    "\n",
    "def exists(var):\n",
    "    '''\n",
    "    Attempting to check if a variable exists.\n",
    "    Would be better to just reference system/script/etc variables and check for a match.\n",
    "    This implementation is quite insecure and is just a temporary way of error handling.\n",
    "    TODO: If this still exists in 2023, I probably went with another error handling method.\n",
    "    '''\n",
    "    try:\n",
    "        a = exec(var) #lol... TODO: Not this.\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_filter_set(filter_path, filter_library=None):\n",
    "    if filter_library is None:\n",
    "        if exists(\"self.default_filter_dict\"):\n",
    "            # filter_library=default_filter_dict\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                from models.config.filter_methods import testing_platform_filters\n",
    "                filter_library=testing_platform_filters\n",
    "            except:\n",
    "                from .models.config.filter_methods import testing_platform_filters\n",
    "                filter_library=testing_platform_filters\n",
    "    \n",
    "    # parsing_methods = self.default_filter_dict\n",
    "    parsing_methods = filter_library\n",
    "    for slicer in filter_path.split(\".\"):\n",
    "        parsing_methods = parsing_methods[slicer]\n",
    "    return parsing_methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# email_data = parse_emails(file_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {\n",
    "'scan' : {\n",
    "    'date'        : lambda df: df['headers'].apply(lambda x : dt.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime(DATE_FORMAT)), #standard to gmail format; gets datetime of email sent\n",
    "    'update_type' : lambda df: df['headers'].apply(lambda x : re.search(r'jobs?_appli\\w{4,6}_([a-z]+)', x['X-LinkedIn-Template']).group(1)), #type of email update\n",
    "    'subject'     : lambda df: df['headers'].apply(lambda x : decode_mime_stuff(x['Subject']).replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\").split(\":\")[0]),\n",
    "    # 'title' : lambda x : re.search(x,r''), #title of job posting\n",
    "    # 'company' : lambda x : re.search(x,r''), #company name\n",
    "},\n",
    "'applied' : {\n",
    "    'PostID'   : lambda df: df['body'].apply(lambda x : re.search(r\"(\\d{10})\", x).group(1)), #.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1)),\n",
    "    'url'      : lambda df: df['body'].apply(lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\", x).group(1))), #lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\", x.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1))),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[0]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[1]),\n",
    "},\n",
    "'viewed' : {\n",
    "    'PostID'   : lambda df: df['body'].apply(lambda x : re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', flags = re.DOTALL), x).group(1)),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" was viewed by \")[0]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" was viewed by \")[0]),\n",
    "},\n",
    "'rejected' : {\n",
    "    'original_date_applied' : lambda df: df['body'].apply(lambda x : dt.strptime(re.search(re.compile(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\"),x).group(1),'%B %d, %Y').strftime(DATE_FORMAT)),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[0]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[1]),\n",
    "},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local Files\n",
    "file_cache = load_emails('.\\\\data\\\\input_files\\\\')\n",
    "file_df = pd.DataFrame(file_cache)\n",
    "type(file_df.iloc[0]['body'])\n",
    "\n",
    "file_df = file_df.assign(**filter_dict['scan'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_update_results = {}\n",
    "for values in file_df['update_type'].unique():\n",
    "    target_df  = file_df.query(f\"update_type == \\'{values}\\'\")\n",
    "    file_update_results[values] = target_df.assign(**filter_dict[values])\n",
    "file_result_df = pd.concat(file_update_results.values())\n",
    "file_result_df.drop(columns=\"headers\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gmail\n",
    "credential_path = \".\\\\credentials\\\\mika_wisener_gmail_credentials.json\"\n",
    "query=r'from:jobs-noreply@linkedin.com|jobs-listings@linkedin.com -\"apply now|to\" -\"new job|jobs\" -\"don\\'t forget\"'\n",
    "email_cache = getEmails(credpath=credential_path, query=query, maxResults=500) #TODO Add verbose = ['summary', 'all', False] #TODO Can probably get changelog from these vars\n",
    "pd.DataFrame(email_cache)\n",
    "test_df = pd.DataFrame(email_cache)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(**filter_dict['scan'])\n",
    "updates_results = {}\n",
    "for values in test_df['update_type'].unique():\n",
    "    target_df  = test_df.query(f\"update_type == \\'{values}\\'\")\n",
    "    updates_results[values] = target_df.assign(**filter_dict[values])\n",
    "# result_df = updates_results['applied'].merge(updates_results['viewed'],)\n",
    "real_app_history = updates_results['applied']\n",
    "real_app_history.head()\n",
    "real_app_history['date_applied'] = real_app_history['date']\n",
    "real_app_history = pd.merge(updates_results['applied'], updates_results['viewed'][['PostID','date']], how='left', on=\"PostID\", suffixes=[\"\",\"_viewed\"])\n",
    "real_app_history = pd.merge(real_app_history, updates_results['rejected'][['position','company','date']], how='left', on=[\"position\",\"company\"], suffixes=[\"\",\"_rejected\"]).sort_values(by=\"date_applied\", ascending=True, axis=0)\n",
    "\n",
    "\n",
    "# result_df = pd.concat(updates_results.values())\n",
    "real_app_history.drop(columns=[\"headers\",\"date\", \"update_type\"], inplace=True)\n",
    "real_app_history.head()\n",
    "# # email_data = method(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_hist = result_df.sort_values(by=\"date\", ascending=True, axis=0)\n",
    "app_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(email_cache)\n",
    "# test_df = test_df.assign(**testing_platform_filters['LinkedIn']['email']['scan'])\n",
    "\n",
    "\n",
    "# testing_platform_filters['LinkedIn']['email']['scan']\n",
    "# print(**testing_platform_filters['LinkedIn']['email']['scan'])\n",
    "\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['headers'].apply(lambda x: x['X-LinkedIn-Template'])\n",
    "filter_dict = {\n",
    "'scan' : {\n",
    "    'date'        : lambda df: df['headers'].apply(lambda x : dt.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime(DATE_FORMAT)), #standard to gmail format; gets datetime of email sent\n",
    "    'update_type' : lambda df: df['headers'].apply(lambda x : re.search(r'jobs?_appli\\w{4,6}_([a-z]+)', x['X-LinkedIn-Template']).group(1)), #type of email update\n",
    "    'subject'     : lambda df: df['headers'].apply(lambda x : decode_mime_stuff(x['Subject']).replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\").split(\":\")[0]),\n",
    "    # 'title' : lambda x : re.search(x,r''), #title of job posting\n",
    "    # 'company' : lambda x : re.search(x,r''), #company name\n",
    "},\n",
    "'applied' : {\n",
    "    'PostID'   : lambda df: df['body'].apply(lambda x : re.search(r\"(\\d{10})\", x).group(1)), #.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1)),\n",
    "    'url'      : lambda df: df['body'].apply(lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\", x).group(1))), #lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\", x.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1))),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[0].split(\" for \")[1]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[1]),\n",
    "},\n",
    "'viewed' : {\n",
    "    'PostID'   : lambda df: df['body'].apply(lambda x : re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', flags = re.DOTALL), x).group(1)),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" was viewed by \")[0].split(\" for \")[1]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" was viewed by \")[1]),\n",
    "},\n",
    "'rejected' : {\n",
    "    'original_date_applied' : lambda df: df['body'].apply(lambda x : dt.strptime(re.search(re.compile(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\"),x).group(1),'%B %d, %Y').strftime(DATE_FORMAT)),\n",
    "    'position' : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[0].split(\" to \")[1]),\n",
    "    'company'  : lambda df: df['subject'].apply(lambda x : x.split(\" at \")[1]),\n",
    "},\n",
    "}\n",
    "\n",
    "test_df = test_df.assign(**filter_dict['scan'])\n",
    "test_df\n",
    "\n",
    "# test_df['update_type'] = test_df['headers'].apply(lambda x : re.search(r'jobs?_appli\\w{4,6}_([a-z]+)', x['X-LinkedIn-Template']).group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates_results = {}\n",
    "for values in test_df['update_type'].unique():\n",
    "    target_df  = test_df.query(f\"update_type == \\'{values}\\'\")\n",
    "    updates_results[values] = target_df.assign(**filter_dict[values])\n",
    "result_df = pd.concat(updates_results.values())\n",
    "result_df.drop(columns=\"headers\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GMAIL\n",
    "\n",
    "# credential_path = \".\\\\credentials\\\\mika_wisener_gmail_credentials.json\"\n",
    "# query=r'from:jobs-noreply@linkedin.com|jobs-listings@linkedin.com -\"apply now|to\" -\"new job|jobs\" -\"don\\'t forget\"'\n",
    "# email_cache = getEmails(credpath=credential_path, query=query, maxResults=500) #TODO Add verbose = ['summary', 'all', False]\n",
    "\n",
    "\n",
    "\n",
    "# len(email_cache)\n",
    "\n",
    "# parsed_emails = parse_emails(email_cache)#TODO Add verbose = ['summary', 'all', False]\n",
    "# app_history = pd.DataFrame(parsed_emails)\n",
    "\n",
    "# app_history.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datum = app_history.iloc[0]\n",
    "\n",
    "\n",
    "test_func = lambda x: dt.strptime(re.search(re.compile(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\"),x['body'][0].text).group(1),'%B %d, %Y').strftime(DATE_FORMAT)\n",
    "# test_func = lambda x: x['body'][0].find(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\")\n",
    "print(test_func(test_datum))\n",
    "\n",
    "# _testing_result = app_history.iloc[0]['body'][0].find_all(\"a\")\n",
    "# _testing_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .models.Parser import \n",
    "\n",
    "# parsed_features = self.scrape_html(input_data['url'])\n",
    "# post_data = self.parser.scrape_posting(input_data['PostID'], parsing_library=\"html5lib\", filter_path=\"selenium-client.post\")\n",
    "# company_data = self.parser.scrape_company_data(post_data)\n",
    "\n",
    "# data.update(parsed_features)f\n",
    "# data.update(post_data)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from models.Parser import Parser\n",
    "configuration = dotenv_values(\".env\")\n",
    "p = Parser()\n",
    "# more_features = [item for item in app_history]\n",
    "platform = \"LinkedIn\"\n",
    "filter_path=f\"{platform}.selenium-client.post\"\n",
    "# get_filter_set(testing_platform_filters, filter_path=filter_path)\n",
    "\n",
    "\n",
    "\n",
    "#scan data\n",
    "#data.update_type\n",
    "#selenium-client.post\n",
    "#selenium-client.company_url\n",
    "\n",
    "# more_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = email_cache\n",
    "new_data = []\n",
    "filters = get_filter_set(f\"{platform}.email\")\n",
    "from models.config.filter_methods import testing_platform_filters\n",
    "filter_test = testing_platform_filters[f'{platform}']['email']\n",
    "print(\"1:\", filter_test,\"\\n\")\n",
    "print(\"2:\",filters)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# data = [datum|handle_comprehension_errors(datum, filters['scan']) for datum in data]\n",
    "# data = [datum|handle_comprehension_errors(datum, filters[datum['update_type']]) for datum in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCRATCHWORK FOR LAMBDA FUNCTION CREATION\n",
    "\n",
    "Creating a lambda function to convert a string containing a date into the desired date format.\n",
    "\n",
    "Input: dictionary object containing email data\n",
    "Output: formatted date string\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "EXAMPLE INPUT:\n",
    "'''\n",
    "dict_input =  {\n",
    "    'subject': str(None),\n",
    "    'body': BeautifulSoup(\"None\", \"html5lib\"),\n",
    "    'headers': {\n",
    "        'Received' : 'by 2002:a05:6102:31b8:0:0:0:0 with SMTP id d24csp1274775vsh; Fri, 9 Sep 2022 11:37:46 -0700 (PDT)', #Location of date string\n",
    "    },\n",
    "}\n",
    "match_pattern = '%d %b %Y %H:%M:%S %z' #Input date format\n",
    "output_pattern = '%Y-%m-%d %H:%M:%S %Z' #Desired output date format\n",
    "\n",
    "\n",
    "'''\n",
    "FUNCTION DEFINITIONS:\n",
    "Creating step by step lambdas to brainstorm the best order of the one-line solution.\n",
    "The one-liner is made by collapsing each lambda with the one above\n",
    "\n",
    "Ex:\n",
    "\n",
    "lambda x : f(x)\n",
    "lambda y : g(y)\n",
    "lambda z : h(z)\n",
    "\n",
    "becomes\n",
    "\n",
    "lambda x : h(g(f(x)))\n",
    "'''\n",
    "\n",
    "\n",
    "#Individual functions to be chained together\n",
    "input_method = lambda x: x['headers']['Received'].split(';')[1].split(\",\")[1].strip() #Locates and isolates the date string\n",
    "cleaning_method = lambda x : re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x) #Removes time zone identifier. Since we already have the time-zone offset, we can avoid the parsing error where '%Z' doesn't match 'PDT'\n",
    "convert_method = lambda x : dt.strptime(x, match_pattern) #converts string to datetime object\n",
    "output_method = lambda x : x.strftime(output_pattern) #converts datetime object to desired string format.\n",
    "\n",
    "\n",
    "#Parameterized version. Receives a clean string along with matching and desired date formats.\n",
    "date_method = lambda string_input, match_pattern, output_pattern : dt.strptime(string_input, match_pattern).strftime(output_pattern)\n",
    "\n",
    "#All the above as a one-line solution with parameterized formatting:\n",
    "oneliner_method_with_parameters = lambda x : dt.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), match_pattern).strftime(output_pattern)\n",
    "oneliner_method_without_parameters = lambda x : dt.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "\n",
    "'''\n",
    "OPERATIONS\n",
    "'''\n",
    "\n",
    "#Step by step operation:\n",
    "input_test = input_method(dict_input) #Locates and isolates the date string\n",
    "cleaning_test = cleaning_method(input_test) #Formats string for conversion\n",
    "convert_test = convert_method(cleaning_test) #Converts string to a datetime object\n",
    "output_test = output_method(convert_test) #Converts datetime object to desired format\n",
    "\n",
    "#Variations on one-lining:\n",
    "date_test = date_method(cleaning_test, match_pattern, output_pattern) #Converts a cleaned string to a datetime object according to the match_pattern, converts to desired output_pattern. Not used for this project, but has use elsewhere.\n",
    "oneliner_test = oneliner_method_with_parameters(dict_input) #The one-line solution. Hard to read, but functional. This will be stored in our method dictionary.\n",
    "\n",
    "\n",
    "'''\n",
    "OUTPUTS\n",
    "'''\n",
    "\n",
    "print('input_test:\\t',input_test)       #input_test:\t 9 Sep 2022 11:37:46 -0700 (PDT)\n",
    "print('cleaning_test:\\t',cleaning_test) #cleaning_test:\t 9 Sep 2022 11:37:46 -0700\n",
    "print('convert_test:\\t',convert_test)   #convert_test:\t 2022-09-09 11:37:46-07:00\n",
    "print('output_test:\\t',output_test)     #output_test:\t 2022-09-09 11:37:46 UTC-07:00\n",
    "\n",
    "print('date_test:\\t',date_test)         #date_test:      2022-09-09 11:37:46 UTC-07:00\n",
    "print('oneliner_test:\\t',oneliner_test) #oneliner_test:\t 2022-09-09 11:37:46 UTC-07:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "testing_platform_filters\n",
    "\n",
    "input_data = None\n",
    "if input_data(\"platform.email.match\")\n",
    "    _changelog = \"platform.email.scan\"\n",
    "    _application_info = \"platform.email.{}\".format(_changelog[\"Update_Type\"])\n",
    "    if \"LinkedIn\":\n",
    "        _new_results = [method_list for method_list in \"platform.selenium-client\".keys()]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('devenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca60e0285cea80f50fc6ff8d3b9ac5d7103f3bc90a48f0b3a1ef8e26510277bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
