{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import datasets\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import email\n",
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "from models.config.cleaning_methods import decode_mime_stuff\n",
    "import datetime as dt\n",
    "from models.config.filter_methods import testing_platform_filters\n",
    "from models.config.CONSTS import CONSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stripped down version of the platform parsing methods dict.\n",
    "Creating functions to streamline data creation.\n",
    "'''\n",
    "\n",
    "\n",
    "platform_identification = {\n",
    "    \"linkedin.com\":\"LinkedIn\",\n",
    "    \"untapped.gg\":\"Untapped\",\n",
    "    \"ziprecruiter.com\":\"ZipRecruiter\",\n",
    "    \"squarepeg.com\":\"SquarePeg\",\n",
    "}\n",
    "\n",
    "platform_filters = {\n",
    "    'LinkedIn' : {\n",
    "        'email': {\n",
    "            'match' : lambda x : \"X-LinkedIn-Template\" in x['headers'].keys(),\n",
    "            'scan' : {\n",
    "                'date'        : lambda x : dt.datetime.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime(CONSTS['date_format']), #standard to gmail format; gets datetime of email sent\n",
    "                'update_type' : lambda x : re.search(r'jobs?_appli\\w{4,6}_([a-z]+)', x['headers']['X-LinkedIn-Template']).group(1), #type of email update\n",
    "                'subject'     : lambda x : decode_mime_stuff(x['headers']['Subject']).replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\").split(\":\")[0],\n",
    "                # 'title' : lambda x : re.search(x,r''), #title of job posting\n",
    "                # 'company' : lambda x : re.search(x,r''), #company name\n",
    "            },\n",
    "            'applied' : {\n",
    "                'PostID'   : lambda x : re.search(r\"(\\d{10})\",x['body'].find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "                'url'      : lambda x : 'https://www.linkedin.com/jobs/view/{}/'.format(re.search(r\"(\\d{10})\",x['body'].find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1)),\n",
    "                'position' : lambda x : x['subject'].split(\" at \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" at \")[1],\n",
    "            },\n",
    "            'viewed' : {\n",
    "                'PostID'   : lambda x : re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim'), x['body'].find(\"a\", href=re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim'))['href']).group(1), #re.search(re.compile(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', flags = re.DOTALL), x['body']).group(1),\n",
    "                'position' : lambda x : x['subject'].split(\" was viewed by \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" was viewed by \")[1],\n",
    "            },\n",
    "            'rejected' : {\n",
    "                # 'original_date_applied' : lambda x: dt.datetime.strptime(re.search(re.compile(r\"Applied on (\\w{3,9} \\d{1,3}, \\d{4})\"), x['body']).group(1),'%B %d, %Y').strftime(CONSTS['date_format']),\n",
    "                'position' : lambda x : x['subject'].split(\" at \")[0],\n",
    "                'company'  : lambda x : x['subject'].split(\" at \")[1],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'TEMPLATE' : {\n",
    "        'email': {\n",
    "            'match' : lambda x: x,\n",
    "            'scan' : {\n",
    "                'date'        : lambda x: x,\n",
    "                'update_type' : lambda x: x,\n",
    "                'subject'     : lambda x: x,\n",
    "                # 'title' : lambda x: x,\n",
    "                # 'company' : lambda x: x,\n",
    "            },\n",
    "            'applied' : {\n",
    "                'PostID'   : lambda x: x,\n",
    "                'url'      : lambda x: x,\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "            'viewed' : {\n",
    "                'PostID'   : lambda x: x,\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "            'rejected' : {\n",
    "                'position' : lambda x: x,\n",
    "                'company'  : lambda x: x,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "def cleaning_function_decorator(func):\n",
    "    '''\n",
    "    Wrapper that cleans all strings inside a list, dict, or string-like object.\n",
    "    Converts numbers into float or int if applicable.\n",
    "    Returns an object of the same format, but with cleaned strings.\n",
    "    \n",
    "    '''\n",
    "    def wrapper(*args, **kwargs):\n",
    "        results = func(*args,**kwargs)\n",
    "        return clean_data(results)\n",
    "    \n",
    "    def clean_string(text):\n",
    "        replace_chars = [('\\n',\" \"), (re.compile(r\"\\s+\"),\" \")]\n",
    "        results = text\n",
    "        for item in replace_chars:\n",
    "            results = re.sub(item[0],item[1], results)\n",
    "        results = results.strip()\n",
    "        return results\n",
    "    \n",
    "    def clean_data(input_item):\n",
    "        '''\n",
    "        Recursive function that cleans strings inside objects.\n",
    "        '''\n",
    "        type_list = {\"int\":int, \"str\":str, \"dict\":dict, \"list\":list,}\n",
    "        result_type = max([label if isinstance(input_item, type_example) else \"\" for (label,type_example) in type_list.items()]) #creates a list that checks the type of the input and returns a string representation of the type. Mostly using this so I can use match/case instead of an If/Else ladder.\n",
    "        match result_type:\n",
    "            case \"str\":\n",
    "                print(\"string\")\n",
    "                if len(input_item)<256 and re.search(r\"\\d\",input_item): #Typically not working with bigint, but will change as needed.\n",
    "                    try: #This is going to hurt performance, but this will allow numerical strings to be relabeled as int or float\n",
    "                        try:\n",
    "                            return int(input_item.strip(\",\"))\n",
    "                        except:\n",
    "                            return float(input_item.strip(\",\"))\n",
    "                    except:\n",
    "                        return clean_string(input_item)\n",
    "                return clean_string(input_item)\n",
    "            case \"dict\":\n",
    "                print(\"dictionary\")\n",
    "                input_item = {label:clean_data(item) for (label,item) in input_item.items()}\n",
    "            case \"list\":\n",
    "                print(\"list\")\n",
    "                return [clean_data(item) for item in input_item]\n",
    "            case \"int\":\n",
    "                print(\"int\")\n",
    "            case \"float\":\n",
    "                print(\"float\")\n",
    "            case _:\n",
    "                print(result_type)\n",
    "        return input_item\n",
    "    return wrapper\n",
    "\n",
    "def handle_comprehension_errors(input_data, method_list):\n",
    "    if \"errors\" not in input_data.keys():\n",
    "        input_data |= {\"errors\":{}}\n",
    "    for data_label in method_list: #loops through each data_label \n",
    "        try:\n",
    "            input_data.update({data_label : method_list[data_label](input_data)}) #applies filter to captured data and inserts the key-value pair into input_data\n",
    "        except Exception as e:\n",
    "            input_data.update({data_label : None}) #If error, fills in value with null\n",
    "            input_data[\"errors\"].update({data_label:e}) #\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def load_emails(directory):\n",
    "    cached_emails = []\n",
    "    file_list = os.listdir(directory)\n",
    "    file_list = [file for file in file_list if \".eml\" in file] #file not in read_files and \n",
    "    for item in file_list:\n",
    "        path = directory+item\n",
    "        with open(path, encoding=\"utf-8\") as file:\n",
    "            results = email.message_from_file(file)\n",
    "            if results.is_multipart:\n",
    "                # results = results.get_payload()\n",
    "                body = results.get_payload()[1]\n",
    "                print(\"Multipart\")\n",
    "            else:\n",
    "                # results = results.get_payload()\n",
    "                body = results.get_payload()\n",
    "                print(\"Not Multipart\")\n",
    "            # print(results)\n",
    "            body = re.sub(r\"=$\\n\",\"\", body.as_string(), flags=re.MULTILINE)\n",
    "            headers = {part:re.sub(r\"\\n\\s+\",\" \",value) for (part,value) in results.items()}\n",
    "            html_contents = BeautifulSoup(body, \"html5lib\")\n",
    "            data = { \n",
    "                'subject': headers['Subject'],\n",
    "                'headers': headers,\n",
    "                'body': html_contents,\n",
    "                'from': headers['From']\n",
    "            }\n",
    "            \n",
    "            cached_emails.append(data)\n",
    "    return cached_emails\n",
    "\n",
    "    \n",
    "            # print(headers.keys())\n",
    "\n",
    "def parse_email(data):\n",
    "    input_method = 'email'\n",
    "\n",
    "    # print(data['headers']['Subject'])\n",
    "    sender = data['from'].split(\"@\")[1].strip(\">\")\n",
    "    platform = platform_identification[sender] if sender in platform_identification.keys() else \"Unknown\"\n",
    "    # platform = 'LinkedIn'\n",
    "    filters = platform_filters[platform][input_method]\n",
    "    # filters = testing_platform_filters[platform][input_method]\n",
    "    \n",
    "    try:\n",
    "        if filters['match'](data):\n",
    "            email_data = {label:method(data) for (label,method) in filters['scan'].items()} #Initializes date, update_type, and subject\n",
    "            email_data |= {label:method(email_data) for (label,method) in filters[data['update_type']].items()} #processes the above for more data\n",
    "    except Exception as e:\n",
    "        print(e, \"!!!\", data['subject'])\n",
    "        print(\"Body: \", data['body'])\n",
    "    return data\n",
    "\n",
    "\n",
    "@cleaning_function_decorator\n",
    "def parse_emails(input_data):\n",
    "    results = [parse_email(item) for item in input_data]\n",
    "    return results\n",
    "\n",
    "\n",
    "    '''\n",
    "    data = {\n",
    "        \"date:\":headers['Received'].split(\";\")[1],\n",
    "        \"update_type\": re.search(r'jobs?_appli\\w{4,6}_([a-z]+)',headers['X-LinkedIn-Template']).group(1), \n",
    "        \"JobID\": re.search(r\"(\\d{10})\",html_contents.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "        \"Subject\": headers['Subject'].replace(\",\",\"\").replace(\".\",\"\").replace(\"&\", \"and\"), #re.search(r\"Your? appli(cation|ed) (for|to) (.+)( at|was viewed by)(.+)\",headers['Subject']), \n",
    "        \"From\":headers['From'],\n",
    "        \"To\":headers['To'],\n",
    "        # \"Body\":body,\n",
    "    }\n",
    "    # match data['update_type']:\n",
    "    #     case 'applied':\n",
    "    #         print(body)\n",
    "    #         jobID = re.search( r'View job: https:(\\S+&jobId=3D(\\w+))' , html_contents, flags = re.DOTALL ).group(2)\n",
    "    #     case 'viewed':\n",
    "    #         jobID = re.search(r'jobPostingId%3D(\\d{10})%26pivotType%3Dsim', html_contents, flags = re.DOTALL).group(1)\n",
    "    #     case 'rejected':\n",
    "    #         jobID = None\n",
    "    #         # find jobID via match, perhaps. Maybe at later stage?\n",
    "    #     case _:\n",
    "    #         jobID = None\n",
    "            \n",
    "        \n",
    "\n",
    "    # subject_filtering = re.search(re.compile(\"Your? appli(cation|ed) (for|to) (.+)( at|was viewed by)(.+)\",flags=re.DOTALL),decode_mime_stuff(headers['Subject']))\n",
    "    split_string = \" was viewed by \" if data['update_type']==\"viewed\" else \" at \"\n",
    "    position, company = decode_mime_stuff(headers['Subject']).split(split_string)\n",
    "    more_data = {\n",
    "        \"company\": company, #subject_filtering.group(5),\n",
    "        \"position\": position, #subject_filtering.group(3),\n",
    "        \"PostURL\": 'https://www.linkedin.com/jobs/view/{}/'.format(data['JobID'])\n",
    "    }\n",
    "    # \"JobID\": re.search(r\"(\\d{10})\",html_contents.find(\"a\", href=re.compile(r\"https://www.linkedin.com/comm/jobs/view/\"))['href']).group(1),\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cleaning_function_decorator\n",
    "def test_function(input_dict):\n",
    "    # print(input_dict)\n",
    "    return input_dict\n",
    "\n",
    "test_results = [{\"Test\":[\"123\",\"420.69\", \"Dick123\"],\"Frog\":\"15\", \"Returns4\":lambda x: \"4\",\"4\":\"12\"}, {\"Frog\":\"12\"} ]\n",
    "\n",
    "cleaned_results = test_function(test_results)\n",
    "try:\n",
    "    assert isinstance(cleaned_results[0][\"Test\"][1], float), \"FAILED: Float not converted\"\n",
    "    assert isinstance(cleaned_results[0][\"Test\"][0], int), \"FAILED: Int not converted\"\n",
    "    assert cleaned_results[0]['Returns4'](9999)==\"4\", \"FAILED: Lambda converted\"\n",
    "    print(\"PASSED\")\n",
    "    print(cleaned_results)\n",
    "except AssertionError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import email\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "def getEmails(credpath, filters = None, maxRequests = None, query=None, nextPage=None, maxResults=None):\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    maxResults=500 if maxResults is None else maxResults\n",
    "    cached_data = []\n",
    "    duplicates = {}\n",
    "    rejected_values = []\n",
    "    # Variable creds will store the user access token.\n",
    "    # If no valid token found, we will create one.\n",
    "    creds = None\n",
    "    pickle_path = '.\\\\dev\\\\models\\\\config\\\\token.pickle'\n",
    "    pickle_path = re.search(r\"(\\.(\\\\\\w+)+)\\\\\\w+.json\", credpath).group(1)+r\"\\\\token.pickle\"\n",
    "    # The file token.pickle contains the user access token.\n",
    "    # Check if it exists\n",
    "    if os.path.exists(pickle_path):\n",
    "\n",
    "        # Read the token from the file and store it in the variable creds\n",
    "        with open(pickle_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If credentials are not available or are invalid, ask the user to log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(f'{credpath}', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        # Save the access token in token.pickle file for the next run\n",
    "        with open(pickle_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    # Connect to the Gmail API\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "    # request a list of all the messages\n",
    "    if nextPage==None:\n",
    "        result = service.users().messages().list(userId='me', maxResults=maxResults, q=query).execute()\n",
    "    else:\n",
    "        result = service.users().messages().list(userId='me', maxResults=maxResults, q=query, pageToken=nextPage).execute()\n",
    "    \n",
    "    # We can also pass maxResults to get any number of emails. Like this:\n",
    "    # result = service.users().messages().list(maxResults=200, userId='me').execute()\n",
    "    messages = result.get('messages')\n",
    "    number_messages = len(messages)\n",
    "    print(\"Number of messages for this round: \", number_messages)\n",
    "    # messages is a list of dictionaries where each dictionary contains a message id.\n",
    "\n",
    "            \n",
    "    # iterate through all the messages\n",
    "    for msg in messages:\n",
    "        # Get the message from its id\n",
    "        txt = service.users().messages().get(userId='me', id=msg['id']).execute()\n",
    "        # Use try-except to avoid any Errors\n",
    "        # try:\n",
    "        # Get value of 'payload' from dictionary 'txt'\n",
    "        payload = txt['payload']\n",
    "        headers = payload['headers']\n",
    "        headers = {item['name']:item['value'] for item in headers}\n",
    "        # Look for Subject and Sender Email in the headers\n",
    "        filter_stuff = 'X-LinkedIn-Template' in headers.keys() #eventually call all email classification methods #TODO undo hardcoded linkedin\n",
    "        target_values = ['jobs_applicant_applied', 'email_jobs_job_application_viewed_01', 'email_jobs_application_rejected_01']\n",
    "        if filter_stuff: # Checks if \n",
    "            if headers['X-LinkedIn-Template'] not in target_values:\n",
    "                rejected_values.append(headers['X-LinkedIn-Template'])\n",
    "                continue\n",
    "            print(\"FILTER RESULT\", headers['X-LinkedIn-Template'])\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        subject = headers['Subject']\n",
    "        sender = headers['From']\n",
    "        \n",
    "        # The Body of the message is in Encrypted format. So, we have to decode it.\n",
    "        # Get the data and decode it with base 64 decoder.\n",
    "        parts = payload.get('parts')[0]\n",
    "        data = parts['body']['data']\n",
    "        data = data.replace(\"-\",\"+\").replace(\"_\",\"/\")\n",
    "        decoded_data = base64.b64decode(data)\n",
    "\n",
    "        # Now, the data obtained is in lxml. So, we will parse\n",
    "        # it with BeautifulSoup library\n",
    "        soup = BeautifulSoup(decoded_data , \"lxml\")\n",
    "        body = soup.body()\n",
    "\n",
    "        # Printing the subject, sender's email and message\n",
    "        # wrapped_data = [headers,subject,sender,body]\n",
    "        # print(\"Headers: \", headers)\n",
    "        # print(\"Subject: \", subject)\n",
    "        # print(\"From: \", sender)\n",
    "        # print(\"Message: \", body)\n",
    "        # print('\\n')\n",
    "        data = {\n",
    "            \"date_scanned\":dt.now(),\n",
    "            \"subject\":subject,\n",
    "            \"headers\":headers,\n",
    "            \"body\":body,\n",
    "            \"from\":sender,\n",
    "        }\n",
    "        \n",
    "        cached_data.append(data)\n",
    "    # except Exception as e:\n",
    "    # \tprint(e,'\"EXCEPTION')\n",
    "    rejected_values = set(rejected_values)\n",
    "    # print(cached_data)\n",
    "    print(\"Duplicates\", duplicates)\n",
    "    print(\"Unfiltered Emails\", rejected_values)\n",
    "    if number_messages == maxResults: #capped query; need to rerun until all messages are parsed.\n",
    "        print(\"Next round of parsing!\")\n",
    "        cached_data.extend(getEmails(credpath=credpath, filters=filters, query=query, nextPage=result['nextPageToken'], maxResults=maxResults)) # Creating a union between these sets. Allows for recursion.\n",
    "    \n",
    "    return cached_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_path = \".\\\\models\\\\config\\\\mika_wisener_gmail_credentials.json\"\n",
    "query=r'from:jobs-noreply@linkedin.com|jobs-listings@linkedin.com -\"apply now|to\" -\"new job|jobs\" -\"don\\'t forget\"'\n",
    "email_cache = getEmails(credpath=credential_path, query=query, maxResults=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local Files\n",
    "email_data = load_emails('.\\\\data\\\\test_input_files\\\\')\n",
    "\n",
    "email_data = parse_emails(email_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cloud\n",
    "parsed_emails = parse_emails(email_cache)\n",
    "app_history = pd.DataFrame(parsed_emails)\n",
    "app_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .models.Parser import \n",
    "\n",
    "# parsed_features = self.scrape_html(input_data['url'])\n",
    "# post_data = self.parser.scrape_posting(input_data['PostID'], parsing_library=\"html5lib\", filter_path=\"selenium-client.post\")\n",
    "# company_data = self.parser.scrape_company_data(post_data)\n",
    "\n",
    "# data.update(parsed_features)f\n",
    "# data.update(post_data)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from models.Parser import Parser\n",
    "configuration = dotenv_values(\".env\")\n",
    "p = Parser()\n",
    "more_features = [p.add_features(item) for item in app_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCRATCHWORK FOR LAMBDA FUNCTION CREATION\n",
    "\n",
    "Creating a lambda function to convert a string containing a date into the desired date format.\n",
    "\n",
    "Input: dictionary object containing email data\n",
    "Output: formatted date string\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "EXAMPLE INPUT:\n",
    "'''\n",
    "dict_input =  {\n",
    "    'subject': str(None),\n",
    "    'body': BeautifulSoup(\"None\", \"html5lib\"),\n",
    "    'headers': {\n",
    "        'Received' : 'by 2002:a05:6102:31b8:0:0:0:0 with SMTP id d24csp1274775vsh; Fri, 9 Sep 2022 11:37:46 -0700 (PDT)', #Location of date string\n",
    "    },\n",
    "}\n",
    "match_pattern = '%d %b %Y %H:%M:%S %z' #Input date format\n",
    "output_pattern = '%Y-%m-%d %H:%M:%S %Z' #Desired output date format\n",
    "\n",
    "\n",
    "'''\n",
    "FUNCTION DEFINITIONS:\n",
    "Creating step by step lambdas to brainstorm the best order of the one-line solution.\n",
    "The one-liner is made by collapsing each lambda with the one above\n",
    "\n",
    "Ex:\n",
    "\n",
    "lambda x : f(x)\n",
    "lambda y : g(y)\n",
    "lambda z : h(z)\n",
    "\n",
    "becomes\n",
    "\n",
    "lambda x : h(g(f(x)))\n",
    "'''\n",
    "\n",
    "\n",
    "#Individual functions to be chained together\n",
    "input_method = lambda x: x['headers']['Received'].split(';')[1].split(\",\")[1].strip() #Locates and isolates the date string\n",
    "cleaning_method = lambda x : re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x) #Removes time zone identifier. Since we already have the time-zone offset, we can avoid the parsing error where '%Z' doesn't match 'PDT'\n",
    "convert_method = lambda x : dt.datetime.strptime(x, match_pattern) #converts string to datetime object\n",
    "output_method = lambda x : x.strftime(output_pattern) #converts datetime object to desired string format.\n",
    "\n",
    "\n",
    "#Parameterized version. Receives a clean string along with matching and desired date formats.\n",
    "date_method = lambda string_input, match_pattern, output_pattern : dt.datetime.strptime(string_input, match_pattern).strftime(output_pattern)\n",
    "\n",
    "#All the above as a one-line solution with parameterized formatting:\n",
    "oneliner_method_with_parameters = lambda x : dt.datetime.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), match_pattern).strftime(output_pattern)\n",
    "oneliner_method_without_parameters = lambda x : dt.datetime.strptime(re.sub(r\"\\s\\(\\w{3}\\)\",\"\", x['headers']['Received'].split(';')[1].split(\",\")[1].strip()), '%d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "\n",
    "'''\n",
    "OPERATIONS\n",
    "'''\n",
    "\n",
    "#Step by step operation:\n",
    "input_test = input_method(dict_input) #Locates and isolates the date string\n",
    "cleaning_test = cleaning_method(input_test) #Formats string for conversion\n",
    "convert_test = convert_method(cleaning_test) #Converts string to a datetime object\n",
    "output_test = output_method(convert_test) #Converts datetime object to desired format\n",
    "\n",
    "#Variations on one-lining:\n",
    "date_test = date_method(cleaning_test, match_pattern, output_pattern) #Converts a cleaned string to a datetime object according to the match_pattern, converts to desired output_pattern. Not used for this project, but has use elsewhere.\n",
    "oneliner_test = oneliner_method_with_parameters(dict_input) #The one-line solution. Hard to read, but functional. This will be stored in our method dictionary.\n",
    "\n",
    "\n",
    "'''\n",
    "OUTPUTS\n",
    "'''\n",
    "\n",
    "print('input_test:\\t',input_test)       #input_test:\t 9 Sep 2022 11:37:46 -0700 (PDT)\n",
    "print('cleaning_test:\\t',cleaning_test) #cleaning_test:\t 9 Sep 2022 11:37:46 -0700\n",
    "print('convert_test:\\t',convert_test)   #convert_test:\t 2022-09-09 11:37:46-07:00\n",
    "print('output_test:\\t',output_test)     #output_test:\t 2022-09-09 11:37:46 UTC-07:00\n",
    "\n",
    "print('date_test:\\t',date_test)         #date_test:      2022-09-09 11:37:46 UTC-07:00\n",
    "print('oneliner_test:\\t',oneliner_test) #oneliner_test:\t 2022-09-09 11:37:46 UTC-07:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "testing_platform_filters\n",
    "\n",
    "input_data = None\n",
    "if input_data(\"platform.email.match\")\n",
    "    _changelog = \"platform.email.scan\"\n",
    "    _application_info = \"platform.email.{}\".format(_changelog[\"Update_Type\"])\n",
    "    if \"LinkedIn\":\n",
    "        _new_results = [method_list for method_list in \"platform.selenium-client\".keys()]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('devenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca60e0285cea80f50fc6ff8d3b9ac5d7103f3bc90a48f0b3a1ef8e26510277bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
