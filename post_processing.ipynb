{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Parser import Parser\n",
    "from models.Local_Database import Local_Database\n",
    "from models.config import cleaning_methods, filter_methods\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from functools import cache\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_rows', 10)\n",
    "def export_df(df, file_path):\n",
    "    df.to_csv(f\"{file_path}_df.csv\", mode=\"w\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 24\n",
      "[WDM] - Downloading: 100%|██████████| 6.58M/6.58M [00:00<00:00, 32.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "p = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.load_from_backup_csv(\".\\\\dev\\\\data\\\\output_files\\\\db\\\\\")\n",
    "# p.db.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "applicants = p.db['applicant']\n",
    "companies = p.db['companies']\n",
    "rejected = p.db['rejected']\n",
    "viewed = p.db['viewed']\n",
    "\n",
    "applicants['company']=applicants['company'].str.strip(\".\")\n",
    "rejected['company']=rejected['company'].str.strip(\".\")\n",
    "viewed['company']=viewed['company'].str.strip(\".\")\n",
    "\n",
    "companies['Company_Name']=companies['Company_Name'].str.title()\n",
    "applicants['company']=applicants['company'].str.title()\n",
    "rejected['company']=rejected['company'].str.title()\n",
    "viewed['company']=viewed['company'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contract', 'Mid-Senior level', '1,001-5,000 employees', 'IT Services and IT Consulting']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# print(*applicants['Position_Tags'], sep=\"\\n\")\n",
    "test_entry = applicants.iloc[0]\n",
    "print(test_entry['Position_Tags'])\n",
    "print(type(test_entry['Position_Tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies[companies['Company_Name'].isna()]\n",
    "def extract_domain(s):\n",
    "    filter =re.compile(r'https?://w{0,3}\\.?(([A-Za-z0-9-]+\\.?)+[A-Za-z0-9-]+\\.?)')\n",
    "    results = re.search(filter, s)\n",
    "    if results:\n",
    "        results = results.group(1)\n",
    "    return results\n",
    "# extract_domain = lambda x: re.search(re.compile(r'https?://www\\.[A-Za-z0-9-]+\\.[A-Za-z0-9-]+'),x)\n",
    "companies['company_domain']=companies[['Company_Website']].applymap(extract_domain, na_action='ignore')\n",
    "companies[['Company_Name', 'Company_Website', 'company_domain']]\n",
    "# unique_companies = companies['company_domain'].unique()\n",
    "unique_companies = companies['Company_Name'].unique()\n",
    "print(unique_companies)\n",
    "\n",
    "chunk_companies = lambda n: [list(unique_companies[i:min(i+n,int(len(unique_companies)-1)//(n+1))]) for i in range(0,len(unique_companies)//n,n+1)]\n",
    "results = chunk_companies(15)\n",
    "print(results)\n",
    "# email_filter_results = [\"from:\\\"\"+r'|\"'.join([\"@\"+str(ite).split(\".\")[0] + \".\\\"\" for ite in item if str(ite)!=\"nan\"]) + \" -\\\"jobalerts-noreply@linkedin.com\\\" -\\\"jobs-listings@linkedin.com\\\" -\\\"jobs-noreply@linkedin.com\\\"\" for item in results]\n",
    "email_filter_results = [\"\\\"\"+r'|\"'.join([str(ite) + \"\\\"\" for ite in item if str(ite)!=\"nan\"]) + \" -\\\"jobalerts-noreply@linkedin.com\\\" -\\\"jobs-listings@linkedin.com\\\" -\\\"jobs-noreply@linkedin.com\\\" -\\\"jobalerts@cybercoders.com\\\" -\\\"jobalerts@\\\" -from:me\" for item in results]\n",
    "# email_filter_results = [\"\\\"\"+r'|\"'.join([str(ite).split(\".\")[0] + \"\\\"\" for ite in item if str(ite)!=\"nan\"]) + \" -\\\"jobalerts-noreply@linkedin.com\\\" -\\\"jobs-listings@linkedin.com\\\" -\\\"jobs-noreply@linkedin.com\\\"\" for item in results]\n",
    "# .split(\".\")[0]\n",
    "for item in email_filter_results:\n",
    "    print(len(item))\n",
    "print(*email_filter_results, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Trying to create email filter queries\n",
    "def chunkify_list(my_list, n):\n",
    "    suffix = \" -\\\"jobalerts-noreply@linkedin.com\\\" -\\\"jobs-listings@linkedin.com\\\" -\\\"jobs-noreply@linkedin.com\\\" -\\\"jobalerts@cybercoders.com\\\" -\\\"jobalerts@\\\" -from:me\" \n",
    "    \n",
    "    common_words = [\"The\", \"Inc\", \"LLC\", \"Solutions\", \"Partners?\", r\"\\sa\\s\", \"\\san\\s\"]\n",
    "    common_words = [f\"\\s?{item}\\s?\" for item in common_words]\n",
    "    pattern=re.compile(r\"|\".join(common_words),re.IGNORECASE)\n",
    "    remove_common = lambda s: pattern.sub(\" \",s).lstrip().rstrip()\n",
    "    \n",
    "    str_fix = lambda x: r\"|\".join(x).replace('.','').replace(',','')\n",
    "    \n",
    "    my_list = [\"\\\"\"+remove_common(str(item))+\"\\\"\" for item in my_list if str(item).lower()!=\"nan\"]\n",
    "    chunks = [my_list[(i+n*i):min(n*(i+1),len(my_list)-1)] for i in range(0,len(my_list)//n+1)]\n",
    "    results = [str_fix(item)+suffix for item in chunks]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*chunkify_list(unique_companies, 20), sep=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_Company_Bridge = applicants[['PostID','company']]\n",
    "Post_Company_Bridge.columns = ['SourceID', 'Company_Name']\n",
    "merged_companies = pd.merge(companies, Post_Company_Bridge, how=\"left\", on=\"SourceID\", suffixes=[\"_original\", \"\"])\n",
    "merged_companies['Company_Name']=merged_companies['Company_Name'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_companies.set_index(\"Company_Name\")\n",
    "export_df(merged_companies, \".\\\\data\\\\output_files\\\\merged_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Company_Name_original', 'SourceID', 'errors',\n",
       "       'Ticker_Symbol', 'Company_Website', 'Overview', 'Info_Tags',\n",
       "       'Locations', 'error_count', 'company_domain', 'Company_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_companies.columns\n",
    "# for index, entry in merged_companies[['Company_Name_x','Company_Name_y']].iterrows():\n",
    "#     print(index, entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_companies = set(rejected['company'].str.strip(\".\").unique())\n",
    "viewed_companies = set(viewed['company'].str.strip(\".\").unique())\n",
    "applied_companies = set(merged_companies['Company_Name'].unique())\n",
    "rejected_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eri', 'Mulberry', 'Orbisky Systems: Space And Data', 'Scout Clinical'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_companies = rejected_companies.union(viewed_companies)\n",
    "update_companies = viewed_companies.union(rejected_companies)\n",
    "# rejected_companies.difference(applied_companies)\n",
    "# viewed_companies.difference(applied_companies)\n",
    "update_companies.difference(applied_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_companies[merged_companies['Company_Name'].str.startswith(\"Triangulum\")]\n",
    "merged_companies[merged_companies['Company_Name'].str.startswith(\"Scout\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout Clinical <= Scout Clinical, Part Of The Meeting Protocol Family\n",
      "Tiger Analytics <= Tiger Analytics - Us\n",
      "Chainalysis <= Chainalysis Inc\n",
      "System1 <= System1, Llc\n",
      "Mulberry <= Mulberry Technology\n",
      "Marga Consulting <= Marga Consulting Llc\n",
      "Geico <= Geico Regional\n"
     ]
    }
   ],
   "source": [
    "def Fix_Company_Names():\n",
    "    pass\n",
    "update_companies.difference(applied_companies)\n",
    "company_comparison = merged_companies[['Company_Name','Company_Name_original']]\n",
    "# company_comparison = merged_companies[merged_companies[[['Company_Name','Company_Name_original']].isin(update_companies.difference(applied_companies))]]\n",
    "rename_dict = {}\n",
    "for index, entry in company_comparison.iterrows():\n",
    "    if entry.hasnans:\n",
    "        continue\n",
    "    replace_companies = [entry['Company_Name'], entry['Company_Name_original']]\n",
    "    # print(replace_companies)\n",
    "    \n",
    "    smaller, larger = min(replace_companies).title().strip(\".\"), max(replace_companies).title().strip(\".\")\n",
    "    if smaller in larger and not smaller==larger:\n",
    "        print(\"{} <= {}\".format(smaller, larger))\n",
    "        rename_dict.update({replace_companies[1]:replace_companies[0]})\n",
    "        \n",
    "# rejected_companies['company'].replace(rename_dict)\n",
    "# viewed_companies['company'].replace(rename_dict)\n",
    "    # smaller, larger = min(compare_me), max(compare_me)\n",
    "    # if smaller != larger:\n",
    "    #     print(\"{} <= {}\".format(smaller, larger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(self,df1,df2, cache): #TODO refactor for current paradigm\n",
    "        # df['new'] = np.where((df1['Company'],df2['Company']) & (df['Title'] <= df['Title']), df['Column1'], None)\n",
    "        columns = cache['applicant'].keys()\n",
    "        df1 = pd.DataFrame(cache['applicant'])['company'].unique()\n",
    "        updated_companies = []\n",
    "        updates = cache.keys().remove('applicant')\n",
    "        for update_type in updates:\n",
    "            updated_companies.append(list(pd.DataFrame(cache[update_type])['company']))\n",
    "        df2 = pd.DataFrame(update_companies).unique()\n",
    "        \n",
    "        uniques = df1.Company.unique()\n",
    "    \n",
    "        print(\"UNIQUE: \\n\",uniques)\n",
    "        matches = {}\n",
    "        no_matches = []\n",
    "        #tries its best to match. Might be unnecessary processing. Should attempt merge on viable entries then go back for the missed ones.\n",
    "        for index,row in df2.iterrows():\n",
    "            matching_company = row['Company']\n",
    "            for df_company in uniques:\n",
    "                companies = [matching_company,df_company]\n",
    "                if min(companies).title() in max(companies).title(): #without lower, fully caps names don't match.\n",
    "                    # print(companies)\n",
    "                    matches[matching_company]=df_company\n",
    "                    break\n",
    "            if matching_company not in matches.keys():\n",
    "                no_matches.append(matching_company)\n",
    "        \n",
    "        print(matches)\n",
    "        df2.replace(matches, inplace=True)\n",
    "        print(df2)\n",
    "        grouped_by = ['Company','Title']\n",
    "        app_history = df1.merge(df2, how='left', on=grouped_by)\n",
    "        newgroups = app_history.groupby(grouped_by)\n",
    "        print(\"APP HISTORY!\\n\",app_history)\n",
    "        print(newgroups.head())\n",
    "        # uniques = df1.Company.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: merge updates onto main df as date_viewed, date_rejected, etc.\n",
    "\n",
    "# PostID Match --> Company Match --> Fix Outliers --> Compare Leftovers (Such as an ERI rejection) \n",
    "\n",
    "# viewed\n",
    "#LEFT JOIN applicant with viewed on \"PostID\"\n",
    "#Validate if stragglers\n",
    "\n",
    "# rejected\n",
    "# Identify mismatches (set difference)\n",
    "# Handle mismatches\n",
    "    # Filter applications by application date +/- 1 day (Will timezones mess this up?)\n",
    "    # Company match attempt\n",
    "    # \n",
    "# Merge entire damn thing\n",
    "\n",
    "# pre_application_history = pd.merge(applicants, viewed[['PostID','date']], how=\"left\", on=\"PostID\", suffixes=[\"_applied\", \"_viewed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eri', 'Orbisky Systems: Space And Data'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_reject = rejected.replace(rename_dict)\n",
    "new_viewed = viewed.replace(rename_dict)\n",
    "# diffs = set(new_reject['original_date_applied']).difference(set(applicants['date']))\n",
    "# diffs = set(new_reject[['position','company']]).union(set(new_viewed['title'])).difference(applied_companies)\n",
    "company_diffs  = set(new_reject['company']).union(set(new_viewed['company'])).difference(set(applicants['company']))\n",
    "position_diffs = set(new_reject['position']).union(set(new_viewed['position'])).difference(set(applicants['position']))\n",
    "\n",
    "company_diffs\n",
    "# rejected\n",
    "# new_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rejected = rejected.replace(rename_dict)\n",
    "new_viewed = viewed.replace(rename_dict)\n",
    "diffs = set(new_reject['company']).union(set(new_viewed['company'])).difference(applied_companies)\n",
    "# diffs\n",
    "# pre_application_history = pd.merge(applicants, new_viewed[['PostID','date']], how=\"left\", on=\"PostID\", suffixes=[\"_applied\", \"_viewed\"])\n",
    "# pre_application_history = pd.merge(applicants, new_viewed[['PostID','date']], how=\"left\", on=\"PostID\", suffixes=[\"_applied\", \"_viewed\"])\n",
    "pre_application_history = pd.merge(applicants, new_viewed[['company','position','date']], how=\"left\", on=['company', 'position'], suffixes=[\"_applied\", \"_viewed\"])\n",
    "pre_application_history = pd.merge(pre_application_history, new_rejected[['company','position','date']], how=\"left\", on=['company', 'position'], suffixes=[\"\", \"_rejected\"]).rename(columns={'date':'date_rejected'}, errors='raise')\n",
    "application_history_columns = ['PostID', 'position', 'company', 'Company_Name', 'Location', 'Workplace_Type', 'Premium-Salary', 'Premium-Skills', 'Premium-Skills_Possessed', 'date_applied', 'date_viewed', 'date_rejected', 'Closed']\n",
    "application_history = pre_application_history[application_history_columns]\n",
    "# application_history['Possessed_Skills']=application_history['Premium-Skills'][int(application_history['Possessed_Skills'])]\n",
    "# total_skills = application_history['Possessed_Skills'].sum()\n",
    "# export_df(application_history, \".\\\\data\\\\test_output_files\\\\application_history.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# col_names = pre_application_history.columns\n",
    "# ['Unnamed: 0','errors','Post_Body']\n",
    "# pre_application_history.sort_values\n",
    "\n",
    "# application_history_columns = ['PostID','position', 'company','Location', 'Workplace_Type', 'Premium-Salary', 'Premium-Skills', 'Premium-Skills_Possessed', 'date_applied', 'date_viewed', 'date_rejected',  'Closed']\n",
    "\n",
    "# application_history = pre_application_history[application_history_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostID</th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Workplace_Type</th>\n",
       "      <th>Premium-Salary</th>\n",
       "      <th>Premium-Skills</th>\n",
       "      <th>Premium-Skills_Possessed</th>\n",
       "      <th>date_applied</th>\n",
       "      <th>date_viewed</th>\n",
       "      <th>date_rejected</th>\n",
       "      <th>Closed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3183794383</td>\n",
       "      <td>Automation Engineer</td>\n",
       "      <td>Acl Digital</td>\n",
       "      <td>ACL Digital</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On-site</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['SQL', 'Python (Programming Language)', 'Micr...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-09-09 01:04:36 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3172677327</td>\n",
       "      <td>Automation Engineer</td>\n",
       "      <td>The Raymond Newell Group, Inc</td>\n",
       "      <td>The Raymond Newell Group, Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On-site</td>\n",
       "      <td>['$115,000/yr - $145,000/yr', 'Employer-provid...</td>\n",
       "      <td>['Microsoft Office', 'Engineering', 'Microsoft...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-09-09 01:10:37 UTC</td>\n",
       "      <td>2022-09-11 01:19:01 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3254118672</td>\n",
       "      <td>Back End Developer</td>\n",
       "      <td>Eclaro</td>\n",
       "      <td>Eclaro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Python (Programming Language)', 'SQL', 'Java...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-09-09 22:30:43 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3243309366</td>\n",
       "      <td>Backend Engineer- Remote</td>\n",
       "      <td>Cybercoders</td>\n",
       "      <td>CyberCoders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>['$100,000/yr - $130,000/yr', 'Employer-provid...</td>\n",
       "      <td>['Python (Programming Language)', 'SQL', 'Micr...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-09-07 23:43:15 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3199936269</td>\n",
       "      <td>BI Analyst - Security and Privacy</td>\n",
       "      <td>Cypress Hcm</td>\n",
       "      <td>Cypress HCM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Microsoft Excel', 'SQL', 'Microsoft Office',...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2022-09-09 22:28:42 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PostID                           position  \\\n",
       "0  3183794383                Automation Engineer   \n",
       "1  3172677327                Automation Engineer   \n",
       "2  3254118672                 Back End Developer   \n",
       "3  3243309366           Backend Engineer- Remote   \n",
       "4  3199936269  BI Analyst - Security and Privacy   \n",
       "\n",
       "                         company                    Company_Name Location  \\\n",
       "0                    Acl Digital                     ACL Digital      NaN   \n",
       "1  The Raymond Newell Group, Inc  The Raymond Newell Group, Inc.      NaN   \n",
       "2                         Eclaro                          Eclaro      NaN   \n",
       "3                    Cybercoders                     CyberCoders      NaN   \n",
       "4                    Cypress Hcm                     Cypress HCM      NaN   \n",
       "\n",
       "  Workplace_Type                                     Premium-Salary  \\\n",
       "0        On-site                                                NaN   \n",
       "1        On-site  ['$115,000/yr - $145,000/yr', 'Employer-provid...   \n",
       "2         Remote                                                NaN   \n",
       "3         Remote  ['$100,000/yr - $130,000/yr', 'Employer-provid...   \n",
       "4         Remote                                                NaN   \n",
       "\n",
       "                                      Premium-Skills  \\\n",
       "0  ['SQL', 'Python (Programming Language)', 'Micr...   \n",
       "1  ['Microsoft Office', 'Engineering', 'Microsoft...   \n",
       "2  ['Python (Programming Language)', 'SQL', 'Java...   \n",
       "3  ['Python (Programming Language)', 'SQL', 'Micr...   \n",
       "4  ['Microsoft Excel', 'SQL', 'Microsoft Office',...   \n",
       "\n",
       "   Premium-Skills_Possessed             date_applied              date_viewed  \\\n",
       "0                       3.0  2022-09-09 01:04:36 UTC                      NaN   \n",
       "1                       4.0  2022-09-09 01:10:37 UTC  2022-09-11 01:19:01 UTC   \n",
       "2                       2.0  2022-09-09 22:30:43 UTC                      NaN   \n",
       "3                       4.0  2022-09-07 23:43:15 UTC                      NaN   \n",
       "4                       9.0  2022-09-09 22:28:42 UTC                      NaN   \n",
       "\n",
       "  date_rejected Closed  \n",
       "0           NaN   True  \n",
       "1           NaN   True  \n",
       "2           NaN   True  \n",
       "3           NaN   True  \n",
       "4           NaN   True  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicants.head()\n",
    "# print(*applicants['Top_Card_Contents'], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_cum(df):\n",
    "    df2=pd.DataFrame()\n",
    "    df2['applied_jobs']=df['applied_count'].cumsum()\n",
    "    df2['viewed_jobs']=df['viewed_count'].cumsum()\n",
    "    df2['rejected_jobs']=df['rejected_count'].cumsum()\n",
    "    df2['active_jobs']=df2['applied_jobs']-(df2['viewed_jobs']+df2['rejected_jobs'])\n",
    "    print(df2)\n",
    "    plot = sns.lineplot(x=df2.index, y=\"active_jobs\", data=df2)\n",
    "    plot.tick_params(axis='x', rotation=90)  \n",
    "def Create_Charts():\n",
    "    ...\n",
    "    df2 = application_history\n",
    "    normal_dates = lambda x: x.date()\n",
    "    df2['date_applied']=pd.to_datetime(df2['date_applied'])\n",
    "\n",
    "    df2['date_viewed']=pd.to_datetime(df2['date_viewed'])\n",
    "\n",
    "    df2['date_rejected']=pd.to_datetime(df2['date_rejected'])\n",
    "    # print(df2['date_viewed'])\n",
    "    # print(df2.head())\n",
    "    # df2['date_applied']=df2['date_applied'].as_datetime.dt.floor('d')\n",
    "    \n",
    "    # update_dates = df2['date_applied'].to_list()+df2['date_viewed'].to_list()+df2['date_rejected'].to_list()\n",
    "    # update_dates = list(df2['date_applied'].unique())+list(df2['date_viewed'].unique())+list(df2['date_rejected'].unique())\n",
    "    # print(update_dates)\n",
    "    # update_dates=[item.date() for item in update_dates]\n",
    "    # print(update_dates)\n",
    "    # ts_df = pd.DataFrame(update_dates,columns=[\"date\"]).dropna()\n",
    "    # print(ts_df.dropna())\n",
    "    # return 0\n",
    "    # ts_df.merge(applied_count,)\n",
    "    # applied_count=df2[['date_applied']].value_counts().rename_axis('date').reset_index('applied_count').sort_values('date')\n",
    "    applied_count=pd.to_datetime(df2['date_applied']).dt.floor('d').value_counts().rename_axis('date').reset_index(name='applied_count').sort_values('date')\n",
    "    viewed_count=pd.to_datetime(df2['date_viewed']).dt.floor('d').value_counts().rename_axis('date').reset_index(name='viewed_count').sort_values('date')\n",
    "    rejected_count=pd.to_datetime(df2['date_rejected']).dt.floor('d').value_counts().rename_axis('date').reset_index(name='rejected_count').sort_values('date')\n",
    "    \n",
    "    #Pseudo-code for automatically labeling inactive applications as rejected\n",
    "    #date_ghosted = max(date_applied, date_viewed) + 30 days if (!(rejected) and (closed) and (date.now()-max(date_applied, date_viewed) > 1 month)) else None\n",
    "    \n",
    "    applied_count['date'] = [item.date() for item in applied_count['date']]\n",
    "    # viewed_count=df2[['date_viewed']].value_counts()\n",
    "    # rejected_count=df2[['date_rejected']].value_counts()\n",
    "    active_jobs = pd.concat([applied_count, viewed_count, rejected_count],join='outer')\n",
    "    active_jobs['date']=pd.to_datetime(active_jobs['date']).dt.normalize()\n",
    "    active_jobs = active_jobs.set_index('date').sort_index().groupby(level=0).sum()\n",
    "    sum_cum(active_jobs)\n",
    "    \n",
    "    \n",
    "    sectors_applied = df2['']\n",
    "    # df2['viewed_count']=pd.to_datetime(df2['date_viewed']).dt.floor('d').value_counts()\n",
    "    # print(df2['viewed_count'])\n",
    "    return 0\n",
    "\n",
    "\n",
    "    df2['date_rejected']=df2['date_rejected'].dt.floor('d')\n",
    "    # applications_per_day = pd.to_datetime(df2['date_applied']).dt.floor('d').value_counts().rename_axis('date').reset_index(name='applied_count').sort_values('date') # Gives time series representation of applications per day\n",
    "    update_dates = set(df2['date_applied'].to_list()+df2['date_viewed'].to_list()+df2['date_rejected'].to_list())\n",
    "\n",
    "    df2\n",
    "    update_dates\n",
    "    plot = sns.barplot(x=\"date\", y=\"applied_count\", data=df2)\n",
    "    plot.tick_params(axis='x', rotation=90)\n",
    "Create_Charts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Website','Headquarters', 'Company size', 'Founded', 'Specialties']\n",
    "def Parse_Info_Tags(entry):\n",
    "    targets = ['Website','Headquarters', 'Industry', 'Company size', 'Founded', 'Specialties']\n",
    "    try:\n",
    "        if isinstance(entry['Info_Tags'],str):\n",
    "            item = entry['Info_Tags'].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            item = item.split(\"', '\")\n",
    "            item = [entry.replace(\"\\'\",\"\") for entry in item if isinstance(entry,str)]\n",
    "        else:\n",
    "            return {target:None for target in targets}\n",
    "    except Exception as e:\n",
    "        print(e, entry)\n",
    "        return {None}\n",
    "    # print(item, sep=\"\\n\")\n",
    "    extra_data = {}\n",
    "    for target in targets:\n",
    "        if target in item:\n",
    "            target_value = item[item.index(target)+1]\n",
    "            \n",
    "            # print(target, target_value)\n",
    "            extra_data|={target:target_value}\n",
    "    if 'Specialties' in extra_data:\n",
    "        extra_data['Specialties']=[text.strip() for text in extra_data['Specialties'].replace(\", and\",\",\").split(',')]\n",
    "    extra_data['Company_Name']=entry['Company_Name']\n",
    "    return extra_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    HQ_Index = item[item.index(\"Headquarters\")+1] if item.index(\"Headquarters\") else None\n",
    "    Industry_Index = item[item.index(\"Industry\")+1] if item.index(\"Industry\") else None\n",
    "    Specialties = item[item.index(\"Specialties\")+1:] if item.index(\"Specialties\") else None\n",
    "\n",
    "\n",
    "def clean_company_info():\n",
    "    modified_database_dict = [Parse_Info_Tags(item) for index,item in companies.iterrows()]\n",
    "    company_features = pd.DataFrame(modified_database_dict)\n",
    "    # company_features.head()\n",
    "    company_info = pd.merge(companies, company_features, how='left', on=\"Company_Name\").drop([\"Unnamed: 0\", \"Info_Tags\"], axis=1)\n",
    "    #TODO encode Specialties with Prime List Compression\n",
    "    return company_info\n",
    "\n",
    "\n",
    "\n",
    "company_info=clean_company_info()\n",
    "company_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(companies['Info_Tags'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "def export_db(db, table, table_name):\n",
    "    # create my-sqlite.db in the current directory using sqlalchemy\n",
    "    import_df = application_history\n",
    "    import_df = table\n",
    "\n",
    "    db = sqlalchemy.create_engine(db) \n",
    "    import_df.to_sql(table_name, con=db)\n",
    "\n",
    "# db.to_sql('timeseries', application_history, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies\n",
    "\n",
    "export_db(db='sqlite:///app_trak.db', table=application_history, table_name=\"application_history\")\n",
    "export_db(db='sqlite:///app_trak.db', table=companies, table_name=\"company_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 476-477: malformed \\N character escape (1323215996.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [37], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 476-477: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A categorical-list compression/encryption algorithm I thought up to (hopefully) reduce space complexity at the expense of time complexity. \n",
    "\n",
    "Let L be an ordered collection of unique elements, with its size |L|=n, an arbitrary natural number.\n",
    "Let f be an injective function from L to P, the set of primes, such that: \n",
    "    f: L_i --> P_i, the ith prime number\n",
    "    f': P_i --> L_i \n",
    "Let M be a random subset of L, where:\n",
    "    0<=|M|<=10\n",
    "Let g be a function defined as:\n",
    "    g: M-->\\N\n",
    "    g(M) = prod([f(m) for each m in M])\n",
    "Since g(M) is a product of primes, then, by the fundamental theorem of arithmetic, its factors are unique.\n",
    "Therefore, \n",
    "    g'(M) = [f'(i) for i in factors(g(M))]\n",
    "    \n",
    "My concerns for this algorithm is that as |L| grows large, the worst-case factorization time complexity grows with it.\n",
    "In some respects, so long as |L| and |M| are relatively small, but the typical string representation of M is large, then it can make sense to compress the list/string into an integer or hexadecimal format.\n",
    "That being said, this algorithm might not be necessary for this code, but it was fun to create.\n",
    "\n",
    "Alternative version may include binary representation of options then encoded into hex, but we'll see.\n",
    "'''\n",
    "\n",
    "\n",
    "def Parse_Skills():\n",
    "    total_skills=application_history['Premium-Skills'].to_list()\n",
    "    number_of_skills=[int(count) if str(count).lower()!=\"nan\" else 0 for count in application_history['Premium-Skills_Possessed']]\n",
    "    owned = []\n",
    "    missing = []\n",
    "    print(len(total_skills), len(number_of_skills))\n",
    "    for skills, quantity in zip(total_skills, number_of_skills):\n",
    "        \n",
    "        skills = skills.strip(\"[\").strip(\"]\").split(\",\")\n",
    "        print(\"Owned: \",quantity,skills[:quantity])\n",
    "        print(\"Not owned: \",skills[quantity:])\n",
    "        owned_skills = skills[:(quantity)]\n",
    "        missing_skills = skills[(quantity):]\n",
    "        owned+=owned_skills\n",
    "        missing+=missing_skills\n",
    "    owned_skills = [skill.replace(r\"'\",\"\").strip() for skill in owned]\n",
    "    missing_skills = [skill.replace(r\"'\",\"\").strip() for skill in missing]\n",
    "    print(\"Owned Skills\", set(owned_skills), \"\\n\")\n",
    "    print(\"Missing Skills\", set(missing_skills))\n",
    "    return owned_skills,missing_skills\n",
    "\n",
    "@cache\n",
    "def generate_primes():\n",
    "    iter = 1\n",
    "    @cache\n",
    "    def next_prime(n):\n",
    "        #gets nth prime\n",
    "        global set_primes\n",
    "        new_prime = -1 #TODO get actual prime generating function.\n",
    "        primes.append(new_prime)\n",
    "        set_primes = set(primes)\n",
    "        yield new_prime\n",
    "    while 1:\n",
    "        iter+=1\n",
    "        result = int(primes[(iter-1)] if (iter-1)<=len(primes) else next_prime(iter))\n",
    "        yield result\n",
    "\n",
    "@cache\n",
    "def factorize(input_number):\n",
    "    @cache\n",
    "    def prime_sieve(value):\n",
    "        for prime in filter(lambda x: x>max(factors), primes):\n",
    "            if input_number not in set_primes:\n",
    "                if input_number%prime==0:\n",
    "                    print(\"FACTOR: \", prime)\n",
    "                    return prime\n",
    "            else:\n",
    "                print(\"INPUT IS PRIME\", input_number)\n",
    "                return input_number\n",
    "    factors = [1]\n",
    "    while input_number not in primes and input_number!=1:\n",
    "        input_number = input_number//max(factors)\n",
    "        factor = prime_sieve(input_number)\n",
    "        factors.append(factor)\n",
    "        print(\"INPUT\", input_number,\"FACTOR: \", factor)\n",
    "        \n",
    "    return factors[1:] #removing our seeded 1 value\n",
    "\n",
    "@cache\n",
    "def load_skills(skill_name):\n",
    "    if skill_name in transformed_skills.keys():\n",
    "        skill_ID = transformed_skills[skill_name]\n",
    "    else:\n",
    "        skill_ID = generate_primes()\n",
    "        transformed_skills.update({skill_name:skill_ID})\n",
    "        revert_skills.update({skill_ID:skill_name})\n",
    "    return skill_ID\n",
    "\n",
    "def prod(input_list):\n",
    "    prod = 1\n",
    "    for item in input_list:\n",
    "        prod*=int(item)\n",
    "    return prod\n",
    "\n",
    "\n",
    "def encode_skills(skill_list):\n",
    "    if isinstance(skill_list,str):\n",
    "        cleaned_string = skill_list.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\", \",\",\")\n",
    "        # print(\"Clean\", cleaned_string)\n",
    "        skill_list = cleaned_string.split(\",\")\n",
    "        # print(\"List\", skill_list, len(skill_list))\n",
    "    if isinstance(skill_list, list):\n",
    "        # print([load_skills(skill_name) for skill_name in skill_list])\n",
    "        # return np.prod([load_skills(skill_name) for skill_name in skill_list])\n",
    "        # print(skill_list)\n",
    "        result = [int(transformed_skills[skill]) for skill in skill_list]\n",
    "        print(\"Converted Skill list:\", result)\n",
    "        result_int = prod(result)\n",
    "        # print(result_int)\n",
    "        return result_int\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def decode_skills(composite_number):\n",
    "    factors = factorize(composite_number)\n",
    "    skill_list = [revert_skills[factor] for factor in factors]\n",
    "    return skill_list\n",
    "\n",
    "\n",
    "owned,missing = Parse_Skills()\n",
    "all_skills = set(owned)|set(missing)\n",
    "primes = [2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97,101,103,107,109,113,127,131,137,139,149,151,157,163,167,173,179,181,191,193,197,199,211,223,227,229,233,239,241,251,257,263,269,271,277,281,283,293,307,311,313,317,331,337,347,349,353,359,367,373,379,383,389,397,401,409,419,421,431,433,439,443,449,457,461,463,467,479,487,491,499,503,509,521,523,541,547,557,563,569,571,577,587,593,599,601,607,613,617,619,631,641,643,647,653,659,661,673,677,683,691,701,709,719,727,733,739,743,751,757,761,769,773,787,797,809,811,821,823,827,829,839,853,857,859,863,877,881,883,887,907,911,919,929,937,941,947,953,967,971,977,983,991,997,1009,1013,1019,1021,1031,1033,1039,1049,1051,1061,1063,1069,1087,1091,1093,1097,1103,1109,1117,1123,1129,1151,1153,1163,1171,1181,1187,1193,1201,1213,1217,1223,1229,1231,1237,1249,1259,1277,1279,1283,1289,1291,1297,1301,1303,1307,1319,1321,1327,1361,1367,1373,1381,1399,1409,1423,1427,1429,1433,1439,1447,1451,1453,1459,1471,1481,1483,1487,1489,1493,1499,1511,1523,1531,1543,1549,1553,1559,1567,1571,1579,1583,1597,1601,1607,1609,1613,1619,1621,1627,1637,1657,1663,1667,1669,1693,1697,1699,1709,1721,1723,1733,1741,1747,1753,1759,1777,1783,1787,1789,1801,1811,1823,1831,1847,1861,1867,1871,1873,1877,1879,1889,1901,1907,1913,1931,1933,1949,1951,1973,1979,1987,1993,1997,1999,2003,2011,2017,2027,2029,2039,2053,2063,2069,2081,2083,2087,2089,2099,2111,2113,2129,2131,2137,2141,2143,2153,2161,2179,2203,2207,2213,2221,2237,2239,2243,2251,2267,2269,2273,2281,2287,2293,2297,2309,2311,2333,2339,2341,2347,2351,2357,2371,2377,2381,2383,2389,2393,2399,2411,2417,2423,2437,2441,2447,2459,2467,2473,2477,2503,2521,2531,2539,2543,2549,2551,2557,2579,2591,2593,2609,2617,2621,2633,2647,2657,2659,2663,2671,2677,2683,2687,2689,2693,2699,2707,2711,2713,2719,2729,2731,2741,2749,2753,2767,2777,2789,2791,2797,2801,2803,2819,2833,2837,2843,2851,2857,2861,2879,2887,2897,2903,2909,2917,2927,2939,2953,2957,2963,2969,2971,2999,3001,3011,3019,3023,3037,3041,3049,3061,3067,3079,3083,3089,3109,3119,3121,3137,3163,3167,3169,3181,3187,3191,3203,3209,3217,3221,3229,3251,3253,3257,3259,3271,3299,3301,3307,3313,3319,3323,3329,3331,3343,3347,3359,3361,3371,3373,3389,3391,3407,3413,3433,3449,3457,3461,3463,3467,3469,3491,3499,3511,3517,3527,3529,3533,3539,3541,3547,3557,3559,3571,3581,3583,3593,3607,3613,3617,3623,3631,3637,3643,3659,3671,3673,3677,3691,3697,3701,3709,3719,3727,3733,3739,3761,3767,3769,3779,3793,3797,3803,3821,3823,3833,3847,3851,3853,3863,3877,3881,3889,3907,3911,3917,3919,3923,3929,3931,3943,3947,3967,3989,4001,4003,4007,4013,4019,4021,4027,4049,4051,4057,4073,4079,4091,4093,4099,4111,4127,4129,4133,4139,4153,4157,4159,4177,4201,4211,4217,4219,4229,4231,4241,4243,4253,4259,4261,4271,4273,4283,4289,4297,4327,4337,4339,4349,4357,4363,4373,4391]\n",
    "set_primes = set(primes)\n",
    "transformed_skills = {skill_name:prime for skill_name, prime in zip(all_skills, generate_primes())}\n",
    "revert_skills = {value:skill_name for skill_name,value in transformed_skills.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in generate_primes():\n",
    "    print(i)\n",
    "    if i >300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entry = application_history.iloc[0]['Premium-Skills']\n",
    "print(\"Test Entry: \", test_entry)\n",
    "print(\"Entry is string: \", isinstance(test_entry,str))\n",
    "test_result = encode_skills(test_entry)\n",
    "print(\"Test Result: \", test_result)\n",
    "decode_test = decode_skills(test_result)\n",
    "print(\"Decode Result: \", sorted(decode_test))\n",
    "print(\"Compare MePlz: \", sorted(test_entry.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\", \",\",\").split(\",\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('devenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca60e0285cea80f50fc6ff8d3b9ac5d7103f3bc90a48f0b3a1ef8e26510277bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
